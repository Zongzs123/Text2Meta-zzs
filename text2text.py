# import torch
# from datasets import load_dataset
# from transformers import (
#     AutoTokenizer,
#     AutoModelForCausalLM,
#     BitsAndBytesConfig,
#     TrainingArguments,
# )
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
# from trl import SFTTrainer
# # , DataCollatorForCompletionOnlyLM

# # --- 1. é…ç½®å‚æ•° ---
# # model_name = "Qwen/Qwen2.5-7B-Instruct"  # è¿™é‡Œæ¢æˆä½ å®é™…ä¸‹è½½çš„æ¨¡å‹è·¯å¾„
# model_name = "/home/zzs/data/inference/qwen/Qwen2.5-7B-Instruct"
# dataset_file = "dataset_train.json"
# output_dir = "./qwen_finetuned_output"

# # --- 2. åŠ è½½æ•°æ®é›† ---
# dataset = load_dataset("json", data_files=dataset_file, split="train")

# # --- 3. åŠ è½½æ¨¡å‹å’Œ Tokenizer (4bit é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜) ---
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.float16,
#     bnb_4bit_use_double_quant=True,
# )

# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# tokenizer.pad_token = tokenizer.eos_token # Qwen çš„ pad token è®¾ç½®

# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     quantization_config=bnb_config,
#     device_map="auto",
#     trust_remote_code=True
# )

# # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜
# model.gradient_checkpointing_enable()
# model = prepare_model_for_kbit_training(model)

# # --- 4. é…ç½® LoRA ---
# peft_config = LoraConfig(
#     r=16,     # LoRA ç§©ï¼Œè¶Šå¤§å‚æ•°è¶Šå¤šï¼Œæ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºï¼Œæ˜¾å­˜æ¶ˆè€—è¶Šå¤§ï¼ˆå»ºè®® 8-64ï¼‰
#     lora_alpha=32,
#     lora_dropout=0.05,
#     bias="none",
#     task_type="CAUSAL_LM",
#     # Qwen çš„æ ¸å¿ƒæ¨¡å—ï¼Œå…¨éƒ¨å¾®è°ƒæ•ˆæœæœ€å¥½
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
# )

# model = get_peft_model(model, peft_config)
# model.print_trainable_parameters() # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡

# # --- 5. å®šä¹‰è®­ç»ƒå‚æ•° ---
# training_args = TrainingArguments(
#     output_dir=output_dir,
#     per_device_train_batch_size=4, # æ˜¾å­˜ä¸å¤Ÿå°±è°ƒå°ï¼Œæ¯”å¦‚ 2 æˆ– 1
#     gradient_accumulation_steps=4, # ç´¯ç§¯æ¢¯åº¦ï¼Œç›¸å½“äºå˜ç›¸å¢å¤§ batch size
#     learning_rate=2e-4,            # LoRA å­¦ä¹ ç‡é€šå¸¸æ¯”å…¨é‡å¾®è°ƒå¤§
#     logging_steps=10,
#     num_train_epochs=3,            # è®­ç»ƒè½®æ•°ï¼Œæ•°æ®å°‘å¯ä»¥é€‚å½“å¢åŠ åˆ° 5-10
#     save_strategy="epoch",
#     fp16=True,                     # å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ
#     optim="paged_adamw_32bit",     # èŠ‚çœæ˜¾å­˜çš„ä¼˜åŒ–å™¨
#     report_to="none"               # ä¸ä¸Šä¼ åˆ° wandb
# )

# # --- 6. è®­ç»ƒ Trainer ---
# trainer = SFTTrainer(
#     model=model,
#     train_dataset=dataset,
#     args=training_args,
#     # tokenizer=tokenizer,
#     peft_config=peft_config,
#     # max_seq_length=512, # æ ¹æ®ä½ çš„æ–‡æœ¬é•¿åº¦è°ƒæ•´ï¼Œè¶Šé•¿è¶Šå æ˜¾å­˜
# )

# # --- 7. å¼€å§‹è®­ç»ƒ ---
# print("å¼€å§‹è®­ç»ƒ...")
# trainer.train()

# # --- 8. ä¿å­˜æ¨¡å‹ ---
# print(f"è®­ç»ƒå®Œæˆï¼Œä¿å­˜æ¨¡å‹è‡³ {output_dir}")
# trainer.model.save_pretrained(output_dir)
# tokenizer.save_pretrained(output_dir)

# import torch
# from datasets import load_dataset
# from transformers import (
#     AutoTokenizer,
#     AutoModelForCausalLM,
#     BitsAndBytesConfig,
#     TrainingArguments,
# )
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
# from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# # ================= é…ç½®åŒºåŸŸ =================
# # æ¨¡å‹è·¯å¾„ (è¯·ç¡®è®¤è·¯å¾„æ— è¯¯)
# model_name = "/home/zzs/data/inference/qwen/Qwen2.5-7B-Instruct"
# # æ•°æ®é›†è·¯å¾„ (è¯·ç¡®ä¿æ­¤å‰å·²ç»è¿è¡Œè¿‡æ•°æ®å¤„ç†è„šæœ¬ç”Ÿæˆäº†è¯¥æ–‡ä»¶)
# dataset_file = "dataset_train.json"
# # è¾“å‡ºè·¯å¾„
# output_dir = "./qwen_finetuned_output1"

# # ================= 1. åŠ è½½æ•°æ®é›† =================
# dataset = load_dataset("json", data_files=dataset_file, split="train")

# # ================= 2. åŠ è½½æ¨¡å‹å’Œ Tokenizer =================
# print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œ Tokenizer...")
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.float16,
#     bnb_4bit_use_double_quant=True,
# )

# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# tokenizer.pad_token = tokenizer.eos_token 

# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     quantization_config=bnb_config,
#     device_map="auto",
#     trust_remote_code=True
# )

# # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
# model.gradient_checkpointing_enable()
# model = prepare_model_for_kbit_training(model)

# # ================= 3. é…ç½® LoRA =================
# peft_config = LoraConfig(
#     r=16, 
#     lora_alpha=32,
#     lora_dropout=0.05,
#     bias="none",
#     task_type="CAUSAL_LM",
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
# )

# model = get_peft_model(model, peft_config)
# print(">>> å¯è®­ç»ƒå‚æ•°é‡:")
# model.print_trainable_parameters()

# # ================= 4. å®šä¹‰æ ¼å¼åŒ–å‡½æ•° (ä¿®å¤ ValueError çš„å…³é”®) =================
# def formatting_prompts_func(example):
#     output_texts = []
#     for messages in example['messages']:
#         # ä½¿ç”¨ Qwen çš„ chat template å°† messages åˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²
#         text = tokenizer.apply_chat_template(
#             messages, 
#             tokenize=False, 
#             add_generation_prompt=False
#         )
#         output_texts.append(text)
#     return output_texts

# # ================= 5. å®šä¹‰ Data Collator (åªè®¡ç®—å›å¤éƒ¨åˆ†çš„ Loss) =================
# # Qwen çš„å›å¤å‰ç¼€é€šå¸¸æ˜¯ "<|im_start|>assistant\n"
# response_template = "<|im_start|>assistant\n"
# collator = DataCollatorForCompletionOnlyLM(
#     response_template=response_template, 
#     tokenizer=tokenizer
# )

# # ================= 6. å®šä¹‰è®­ç»ƒå‚æ•° (é’ˆå¯¹ç²¾ç¡®åº¦ä¼˜åŒ–) =================
# training_args = TrainingArguments(
#     output_dir=output_dir,
#     per_device_train_batch_size=2,  # æ˜¾å­˜å…è®¸çš„è¯å¯æ”¹ä¸º 4
#     gradient_accumulation_steps=4,  # ç´¯è®¡æ¢¯åº¦
#     learning_rate=5e-5,             # ã€å…³é”®ã€‘é™ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢åœ¨ç²¾ç¡®æ•°å€¼é™„è¿‘éœ‡è¡
#     logging_steps=10,
#     num_train_epochs=10,            # ã€å…³é”®ã€‘å¢åŠ è½®æ•°ï¼Œå¼ºåˆ¶æ¨¡å‹è®°ä½æ˜ å°„é€»è¾‘
#     save_strategy="epoch",
#     fp16=True, 
#     optim="paged_adamw_32bit",
#     report_to="none"
# )

# # ================= 7. åˆå§‹åŒ– Trainer =================
# trainer = SFTTrainer(
#     model=model,
#     train_dataset=dataset,
#     args=training_args,
#     peft_config=peft_config,
#     data_collator=collator,                  # ä½¿ç”¨ CompletionOnlyLM
#     formatting_func=formatting_prompts_func, # ã€å…³é”®ã€‘ä¼ å…¥æ ¼å¼åŒ–å‡½æ•°
#     max_seq_length=1024,
# )

# # ================= 8. å¼€å§‹è®­ç»ƒ =================
# print("å¼€å§‹è®­ç»ƒ (Target: High Numerical Precision)...")
# trainer.train()

# # ================= 9. ä¿å­˜æ¨¡å‹ =================
# print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {output_dir}")
# trainer.model.save_pretrained(output_dir)
# tokenizer.save_pretrained(output_dir)

# æœ€ç»ˆå®Œæ•´çš„è®­ç»ƒæ¨¡å‹
import torch
import torch.nn as nn
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# ================= é…ç½®åŒºåŸŸ =================
model_name = "/home/zzs/data/inference/qwen/Qwen2.5-7B-Instruct"
dataset_file = "dataset_train.json"
output_dir = "./qwen_finetuned_weighted" # ä¿®æ”¹è¾“å‡ºè·¯å¾„ä»¥ç¤ºåŒºåˆ«

# --- å…³é”®è¶…å‚æ•° ---
NUMBER_LOSS_WEIGHT = 5.0  # ã€æ ¸å¿ƒã€‘æ•°å­—é”™è¯¯çš„æƒ©ç½šå€æ•° (å»ºè®® 3.0 - 10.0)
LEARNING_RATE = 5e-5      # ä¿æŒè¾ƒä½çš„å­¦ä¹ ç‡
NUM_EPOCHS = 10          # ä¿æŒè¾ƒå¤šè½®æ¬¡

# ================= 1. è‡ªå®šä¹‰ Trainer (å®ç°åŠ æƒ Loss) =================
class NumberWeightedSFTTrainer(SFTTrainer):
    def __init__(self, *args, number_weight=5.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.number_weight = number_weight
        
        # é¢„å¤„ç†ï¼šæ‰¾åˆ°è¯è¡¨ä¸­æ‰€æœ‰åŒ…å«æ•°å­—çš„ Token ID
        print(">>> æ­£åœ¨æ„å»ºæ•°å­— Token ç´¢å¼• (ç”¨äºåŠ æƒ Loss)...")
        vocab = self.tokenizer.get_vocab()
        self.number_token_ids = set()
        for token, id in vocab.items():
            # Qwen çš„ token æœ‰æ—¶æ˜¯ byte ç¼–ç ï¼Œè§£ç åæ£€æŸ¥æ˜¯å¦å«æ•°å­—
            # ç®€å•ç²—æš´çš„æ–¹æ³•ï¼šæ£€æŸ¥ token å­—ç¬¦ä¸²ä¸­æ˜¯å¦å« '0'-'9'
            # æ³¨æ„ï¼šSentencePiece/BPE token å¯èƒ½åŒ…å«å‰ç¼€ï¼Œå¦‚ " 123"
            try:
                # å°è¯•è§£ç  token
                decoded = self.tokenizer.decode([id])
                if any(c.isdigit() for c in decoded):
                    self.number_token_ids.add(id)
            except:
                pass
        
        # å°† set è½¬ä¸º tensor æ–¹ä¾¿åç»­è®¡ç®—ï¼Œç§»è‡³æ¨¡å‹è®¾å¤‡ä¼šåœ¨ compute_loss ä¸­å¤„ç†
        # è¿™é‡Œå…ˆå­˜ä¸ª list
        self.number_token_ids_list = list(self.number_token_ids)
        print(f">>> è¯†åˆ«åˆ° {len(self.number_token_ids)} ä¸ªåŒ…å«æ•°å­—çš„ Tokenã€‚")

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        é‡å†™ Loss è®¡ç®—é€»è¾‘ï¼š
        1. è·å–åŸå§‹ logits å’Œ labels
        2. è®¡ç®—é€å…ƒç´ çš„ CrossEntropyLoss (ä¸æ±‚å¹³å‡)
        3. å¯¹å±äºæ•°å­—çš„ token èµ‹äºˆæ›´é«˜çš„æƒé‡
        4. æ±‚å¹³å‡å¹¶è¿”å›
        """
        labels = inputs.get("labels")
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        # è¿™é‡Œçš„é€»è¾‘å‚è€ƒ CausalLM çš„æ ‡å‡† loss è®¡ç®—
        # Shift so that tokens < n predict n
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        
        # 1. è®¡ç®—åŸå§‹ Loss (reduction='none' æ˜¯å…³é”®ï¼Œæˆ‘ä»¬è¦æ‹¿åˆ°æ¯ä¸ª token çš„ loss)
        loss_fct = nn.CrossEntropyLoss(reduction='none')
        # å±•å¹³è®¡ç®—
        shift_logits = shift_logits.view(-1, self.model.config.vocab_size)
        shift_labels = shift_labels.view(-1)
        
        # æ­¤æ—¶ loss æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [batch_size * seq_len] çš„å‘é‡
        loss = loss_fct(shift_logits, shift_labels)
        
        # 2. æ„å»ºæƒé‡æ©ç  (Weight Mask)
        # åˆå§‹åŒ–æƒé‡ä¸º 1.0
        weights = torch.ones_like(loss)
        
        # æ‰¾åˆ°æ‰€æœ‰ label æ˜¯æ•°å­—çš„åœ°æ–¹
        #ä¸ºäº†åŠ é€Ÿï¼Œæˆ‘ä»¬éœ€è¦å°† self.number_token_ids è½¬ä¸ºå½“å‰ device çš„ tensor
        if not hasattr(self, 'number_token_tensor') or self.number_token_tensor.device != loss.device:
            self.number_token_tensor = torch.tensor(
                self.number_token_ids_list, device=loss.device, dtype=shift_labels.dtype
            )
            
        # åˆ¤æ–­ shift_labels ä¸­çš„å…ƒç´ æ˜¯å¦å­˜åœ¨äº number_token_tensor ä¸­
        # torch.isin æ˜¯æœ€å¿«çš„æ–¹æ³•
        is_number_mask = torch.isin(shift_labels, self.number_token_tensor)
        
        # 3. åº”ç”¨æƒé‡
        # å¦‚æœæ˜¯æ•°å­—ï¼Œæƒé‡è®¾ä¸º self.number_weight (æ¯”å¦‚ 5.0)ï¼Œå¦åˆ™ä¿æŒ 1.0
        weights = torch.where(is_number_mask, self.number_weight, 1.0)
        
        # æ³¨æ„ï¼šDataCollator å¯èƒ½ä¼šæŠŠ pad æˆ–è€… prompt éƒ¨åˆ†çš„ label è®¾ä¸º -100
        # CrossEntropyLoss é»˜è®¤å·²ç»å¿½ç•¥äº† -100 çš„ loss (å˜ä¸º 0)
        # æˆ‘ä»¬è¿™é‡Œä¸éœ€è¦é¢å¤–å¤„ç† -100ï¼Œå› ä¸º 0 * weight è¿˜æ˜¯ 0
        
        # 4. åŠ æƒå¹¶æ±‚å¹³å‡
        weighted_loss = loss * weights
        
        # æ±‚å¹³å‡æ—¶ï¼Œåˆ†æ¯åº”è¯¥æ˜¯æœ‰æ•ˆ token çš„æ•°é‡ï¼Œæˆ–è€…æ˜¯æœ‰æ•ˆæƒé‡ä¹‹å’Œ
        # ä¸ºäº†ä¿æŒè®­ç»ƒç¨³å®šï¼Œå»ºè®®é™¤ä»¥æœ‰æ•ˆ token æ•°é‡ (é -100 çš„æ•°é‡)
        # æˆ–è€…ç›´æ¥ mean()ï¼Œå› ä¸º loss ä¸­éæœ‰æ•ˆä½å·²ç»æ˜¯ 0 äº†
        final_loss = weighted_loss.sum() / (shift_labels != -100).sum()
        
        return (final_loss, outputs) if return_outputs else final_loss

# ================= 2. å¸¸è§„è®¾ç½® (åŒæ–¹æ¡ˆä¸€) =================
dataset = load_dataset("json", data_files=dataset_file, split="train")

print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_quant_type="nf4", 
    bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token 

model = AutoModelForCausalLM.from_pretrained(
    model_name, quantization_config=bnb_config, device_map="auto", trust_remote_code=True
)
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# æ ¼å¼åŒ–å‡½æ•°
def formatting_prompts_func(example):
    output_texts = []
    for messages in example['messages']:
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        output_texts.append(text)
    return output_texts

# Data Collator (æ–¹æ¡ˆä¸€çš„å†…å®¹ï¼Œä¿ç•™)
response_template = "<|im_start|>assistant\n"
collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)

# ================= 3. è®­ç»ƒå‚æ•° =================
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=LEARNING_RATE,    # 5e-5
    logging_steps=10,
    num_train_epochs=NUM_EPOCHS,    # 10
    save_strategy="epoch",
    fp16=True, 
    optim="paged_adamw_32bit",
    report_to="none"
)

# ================= 4. åˆå§‹åŒ–è‡ªå®šä¹‰ Trainer =================
print(f"åˆå§‹åŒ– Trainerï¼Œæ•°å­—æƒé‡è®¾ç½®ä¸º: {NUMBER_LOSS_WEIGHT} å€")
trainer = NumberWeightedSFTTrainer(
    model=model,
    train_dataset=dataset,
    args=training_args,
    peft_config=peft_config,
    data_collator=collator,
    formatting_func=formatting_prompts_func,
    max_seq_length=1024,
    number_weight=NUMBER_LOSS_WEIGHT  # ä¼ å…¥æƒé‡å‚æ•°
)

# ================= 5. å¼€å§‹è®­ç»ƒ =================
print("å¼€å§‹è®­ç»ƒ (Scheme 4: Weighted Loss for Numbers)...")
trainer.train()

print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {output_dir}")
trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

from modelscope import snapshot_download

# ä¸‹è½½æ¨¡å‹åˆ°å½“å‰ç›®å½•ä¸‹çš„ Qwen2.5-7B-Instruct æ–‡ä»¶å¤¹
model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir='./')
print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_dir}")


# æ¨ç†éƒ¨åˆ†
import torch
import re
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# è·¯å¾„è®¾ç½® (è¯·ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„è·¯å¾„)
BASE_MODEL_PATH = "/home/zzs/data/inference/qwen/Qwen2.5-7B-Instruct"  # åŸºåº§æ¨¡å‹
ADAPTER_PATH = "./qwen_finetuned_weighted"     # å¾®è°ƒåçš„æƒé‡
INPUT_FILE = "text_robust_out.txt"           # æ¨¡ç³Šæ–‡æœ¬ (User Input)
TARGET_FILE = "text_robust_in.txt"           # ç²¾ç¡®æ–‡æœ¬ (Ground Truth)

# æµ‹è¯•æ ·æœ¬æ•° (None è¡¨ç¤ºæµ‹è¯•å…¨éƒ¨ï¼Œå»ºè®®å…ˆæµ‹ 50 æ¡çœ‹çœ‹æ•ˆæœ)
TEST_SAMPLES = None

# ç”Ÿæˆå‚æ•°
GEN_CONFIG = {
    "max_new_tokens": 128,  # ä¸éœ€è¦å¤ªé•¿ï¼Œåªè¦æ•°å€¼å‡ºæ¥å°±è¡Œ
    "temperature": 0.1,     # ä½æ¸©ï¼Œä¿è¯ç¨³å®šæ€§
    "top_p": 0.9
}
# ===========================================

def load_data(input_path, target_path):
    with open(input_path, 'r', encoding='utf-8') as f_in, \
         open(target_path, 'r', encoding='utf-8') as f_out:
        inputs = [line.strip() for line in f_in.readlines() if line.strip()]
        targets = [line.strip() for line in f_out.readlines() if line.strip()]
    min_len = min(len(inputs), len(targets))
    return list(zip(inputs[:min_len], targets[:min_len]))

def extract_stiffness(text):
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ–‡æœ¬ä¸­çš„ Stiffness æ•°å€¼
    æ”¯æŒæ ¼å¼: "stiffness of 0.70", "stiffness of 1.61"
    """
    # åŒ¹é… "stiffness of" åé¢ç´§è·Ÿçš„æ•°å­— (æ”¯æŒæ•´æ•°å’Œå°æ•°)
    pattern = r"stiffness of\s*(\d+\.?\d*)"
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            return None
    return None

def analyze_prediction(fuzzy_input, gt_text, pred_text):
    """
    æ™ºèƒ½åˆ†æå‡½æ•°ï¼šå¯¹æ¯”çœŸå®å€¼å’Œé¢„æµ‹å€¼
    """
    gt_val = extract_stiffness(gt_text)
    pred_val = extract_stiffness(pred_text)
    
    result = {
        "status": "Unknown",
        "gt_val": gt_val,
        "pred_val": pred_val,
        "diff": 0.0,
        "msg": ""
    }

    # 1. å¦‚æœæå–å¤±è´¥
    if gt_val is None or pred_val is None:
        result["status"] = "âš ï¸ è§£æå¤±è´¥"
        result["msg"] = "æœªèƒ½ä»æ–‡æœ¬ä¸­æå–åˆ° stiffness æ•°å€¼"
        return result

    # 2. è®¡ç®—å·®å¼‚
    diff = abs(gt_val - pred_val)
    result["diff"] = diff

    # 3. åˆ¤å®šé€»è¾‘
    # åˆ¤å®š A: ç²¾ç¡®å‘½ä¸­ (è¯¯å·®å°äº 0.3 æˆ– ç›¸å¯¹è¯¯å·®å°äº 15%)
    if diff <= 0.5 or (gt_val > 0 and diff / gt_val < 0.3):
        result["status"] = "âœ… ç²¾ç¡®å‘½ä¸­"
        result["msg"] = f"è¯¯å·®ä»… {diff:.2f}"
    
    # åˆ¤å®š B: è¶‹åŠ¿æ­£ç¡® (é‡çº§åˆ¤æ–­)
    # å‡è®¾: Soft < 2.0, Moderate 2.0-5.0, Stiff > 5.0 (æ ¹æ®ä½ çš„æ•°æ®åˆ†å¸ƒè°ƒæ•´)
    elif (gt_val < 2.0 and pred_val < 2.0) or \
         (gt_val > 5.0 and pred_val > 5.0):
        result["status"] = "ğŸ‘Œ è¶‹åŠ¿æ­£ç¡®"
        result["msg"] = f"æ•°å€¼ä¸åŒä½†å¤„äºåŒä¸€å¼ºåº¦åŒºé—´ (GT:{gt_val} vs Pred:{pred_val})"
    
    # åˆ¤å®š C: é”™è¯¯
    else:
        result["status"] = "âŒ åå·®è¾ƒå¤§"
        result["msg"] = f"çœŸå®å€¼ {gt_val}ï¼Œé¢„æµ‹å€¼ {pred_val}ï¼Œå·®å¼‚æ˜æ˜¾"

    return result

# ================= ä¸»ç¨‹åº =================
def main():
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {BASE_MODEL_PATH} ...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model.eval()

    print("ğŸ“š æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®...")
    test_data = load_data(INPUT_FILE, TARGET_FILE)
    if TEST_SAMPLES:
        test_data = test_data[:TEST_SAMPLES]
    
    system_prompt = "You are a specialized mechanical engineer. Your task is to translate qualitative, fuzzy structural descriptions into precise quantitative test parameters."
    
    stats = {"exact": 0, "trend": 0, "fail": 0, "parse_error": 0, "diffs": []}
    
    print("\nğŸš€ å¼€å§‹æ™ºèƒ½è¯„ä¼°...")
    print("="*60)

    for i, (inp, truth) in enumerate(tqdm(test_data)):
        # æ„å»º Prompt
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": inp}
        ]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer([text], return_tensors="pt").to(model.device)

        # æ¨ç†
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                **GEN_CONFIG
            )
        output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        prediction = output.split("assistant")[-1].strip()

        # åˆ†æ
        res = analyze_prediction(inp, truth, prediction)

        # ç»Ÿè®¡
        if "âœ…" in res["status"]: stats["exact"] += 1
        elif "ğŸ‘Œ" in res["status"]: stats["trend"] += 1
        elif "âŒ" in res["status"]: stats["fail"] += 1
        else: stats["parse_error"] += 1
        
        if res["gt_val"] is not None and res["pred_val"] is not None:
            stats["diffs"].append(res["diff"])

        # æ‰“å°éƒ¨åˆ†ç»“æœ (æ¯ 5 æ¡æ‰“å°ä¸€æ¬¡ï¼Œæˆ–è€…æ˜¯é”™è¯¯çš„æ—¶å€™æ‰“å°)
        if i < 5 or "âŒ" in res["status"]:
            print(f"\n[Sample {i+1}]")
            print(f"ğŸ“ è¾“å…¥ç‰¹å¾: {inp[:60]}...") # æˆªæ–­æ˜¾ç¤º
            print(f"ğŸ“Š è¯„ä¼°ç»“æœ: {res['status']} | {res['msg']}")
            if res["status"] == "âŒ åå·®è¾ƒå¤§":
                print(f"   -> GT: {truth[:80]}")
                print(f"   -> PR: {prediction[:80]}")

    print("="*60)
    print("\nğŸ“ˆ æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
    total = len(test_data)
    valid_count = len(stats["diffs"])
    
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"âœ… ç²¾ç¡®å‘½ä¸­: {stats['exact']} ({stats['exact']/total:.1%})")
    print(f"ğŸ‘Œ è¶‹åŠ¿æ­£ç¡®: {stats['trend']} ({stats['trend']/total:.1%})")
    print(f"âŒ åå·®è¾ƒå¤§: {stats['fail']} ({stats['fail']/total:.1%})")
    print(f"âš ï¸ è§£æå¤±è´¥: {stats['parse_error']} (æ— æ³•æå–æ•°å­—)")
    
    if valid_count > 0:
        mae = np.mean(stats["diffs"])
        print(f"\nğŸ”¢ å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f}")
        print("ğŸ’¡ ç»“è®º: MAE è¶Šä½è¶Šå¥½ã€‚å¦‚æœ MAE < 0.5ï¼Œè¯´æ˜æ¨¡å‹åœ¨å·¥ç¨‹ä¸Šæ˜¯å¯ç”¨çš„ã€‚")

if __name__ == "__main__":
    main()